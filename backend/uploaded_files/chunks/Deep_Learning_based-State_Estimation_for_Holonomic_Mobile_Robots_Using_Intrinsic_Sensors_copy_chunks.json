[
  {
    "chunk_id": 1,
    "text": "2021 The 21st International Conference on Control, Automation and Systems (ICCAS 2021) Ramada Plaza Hotel, Jeju, Korea, Oct. 12 \u001815, 2021 Deep Learning based-State Estimation for Holonomic Mobile Robots Using Intrinsic Sensors Dinh Van Nam1and Kim Gon-Woo2\u0003 1Department of Control and Robot Engineering, Chungbuk National University, Cheongju, 28644, South Korea, (quangnam.auto.tech@gmail.com, gwkim@cbnu.ac.kr) 2Department of Intelligent Systems and Robotics, Chungbuk National University, Cheongju, 28644, South Korea, (quangnam.auto.tech@gmail.com, gwkim@cbnu.ac.kr)\u0003Corresponding author Abstract: State estimation is a fundamental component of the navigation system of autonomous mobile robots. Gener- ally, the robot setup is equipped with intrinsic and extrinsic sensors. The state estimators have relied almost on intrinsic sensors such as wheel encoders and inertial measurement units in textureless and structureless environments. This paper will analyze and propose the learning state estimation frameworks for the dead-reckoning of autonomous holonomic ve- hicles based only on intrinsic sensors. First, we review and categories the intrinsic-only estimation problem. Second, we describe the problem formulation using learning-based techniques. Next, the learning inertial-only estimation is presented with several strategies using the deep learning technique. The initial experiment results are analyzed and deployed using a holonomic mobile robot in real-world environments. Keywords: deep learning based-state estimation, inertial navigation system, inertial-only estimation, multi-sensor fusion. 1. INTRODUCTION To deal with the self-navigation problem [1], intel- ligent vehicles such as self-driving cars [2], automated guided vehicles [3] needs to know their position in the environment. In general, the autonomous mobile robot using proprioceptive and exteroceptive sensors to fuse the sensor s information to provide reliable states. The Global Positioning System (GPS) is the most crucial sen- sor for long-term navigation for outdoor applications [1] [4]. However, the GPS-based estimator usually jumps with signi cant error in GPS-denied environments such as under-bridges, tunnels, or high buildings [1] Further- more, GPS is disputed for indoor applications such as in- dustrial factories, underwater [1]. For accurate estimation, the robot is equipped with ex- trinsic sensors such as the Light Detection and Ranging (LiDARs), Radars, and cameras that can perceive its sur- rounding environments [1] [4]. The robot is also imple- mented with intrinsic sensors such as the wheel encoders and the Inertial Measurement Unit (IMU) sensors. Nev- ertheless, the LiDARs and radar sensors cannot operate well in structureless environments like repetition walls in long corridors. Moreover, in textureless environments, visual sensors that are usually sensitive to light illumina- tion and light condition fail to observe visual features [1] [4]. In contrast, intrinsic sensors can provide good accu- racy in short-term navigation because they are immune to their surroundings [5]. Still, the wheel encoders or iner- tial sensors are drifted over time and cannot be corrected by using themself. Generally, the wheel encoders and inertial sensors provide higher frequency about 10 to 20 times than LiDAR and visual sensors [6] [5]. Moreover, inertial sensors can operate in 3D environments and give better results than wheel encoders in rotation drift sce- narios [6] [5]. Therefore, we need technologies to handle the state estimation for mobile robots in extrinsic-denied (a) (b)Fig. 1. The four wheels holonomic mobile robot platform in our experiments. (a) presents the overall mechani- cal structure; (b) describes the electronic hardware. environments. In this paper, we present the learning strategies to deal with the robot localization in the outage environments of the exteroceptive sensors. The comprehensive platform of a holonomic mobile robot is shown in Figure 1. 2. RELATED WORKS The navigation system of autonomous mobile robots operates multi-sensor fusion consists of LiDARs, radars, cameras, odometers and IMUs [1] [7] [4]. Recently, Si- multaneous Localization And Mapping (SLAM) [1] has been an advantage technique for state estimation based on the multi-sensor fusion of camera, LiDAR, and inertial sensor. SLAM has been developed for real-time estima- 978-89-93215-21-2/21/$31.00 ICROS 12 Authorized licensed use limited to: INSTITUTE OF ENGINEERING & MANAGEMENT TRUST. Downloaded on January 02,2026 at 16:17:09 UTC from IEEE Xplore. Restrictions apply. tion systems for two decades[11]. The Bayes ltering in- ference is leveraged to maximum the posterior state prob- ability given the initial state and sensor measurements to fuse multiple sensor information [16]. The fusion technique can be categorized into ltering- based and optimization-based [7] [4]. Extended Kalman Filter (EKF), unscented KF (UKF), particle ltering are popular methods with two-phase prediction and correc- tion. The most high-performance visual-inertial naviga- tion system was based on multi-state constraints EKF, which applied the tightly coupled approach for real-time application [8] [9]. The optimization-based method han- dles the state and sensor information for a long time, pro- viding better accuracy than ltering-based [7][4]. VIN- mono [23] is an excellent open-source visual-inertial state estimation system using factor graph optimization (FGO). LIO-SAM [10] is an accurate estimator using the tightly coupled LiDAR inertial.",
    "word_count": 986
  },
  {
    "chunk_id": 2,
    "text": "The most high-performance visual-inertial naviga- tion system was based on multi-state constraints EKF, which applied the tightly coupled approach for real-time application [8] [9]. The optimization-based method han- dles the state and sensor information for a long time, pro- viding better accuracy than ltering-based [7][4]. VIN- mono [23] is an excellent open-source visual-inertial state estimation system using factor graph optimization (FGO). LIO-SAM [10] is an accurate estimator using the tightly coupled LiDAR inertial. LVI-SAM [11] has re- cently been introduced for the tightly coupled of 3D Li- DAR, camera, and IMU via smoothing localization. Arti cial intelligence (AI) based- approach can be considered the third class of multi-sensor fusion. AI is adopted to learn the uncertainty parameters, which are then joined to the traditional fusion techniques [6] [24] [25] [22]. AI can also be applied immediately to deter- mine the prediction state joined to the fusion techniques [12] [15]. Moreover, AI can only use the end-to-end ap- proach to estimate the robot state without MAP inference [13] [27] [14]. To achieve the best performance for state estimation based on the intrinsic sensor -only, we follow the AI-based technique. 3. METHODOLOGY 3.1 IMU and wheel encoders model In general, a modern IMU sensor can directly provide 9 DoF outputs [4][9][7] which are 3D angular velocity vector ~!b(t), 3D acceleration vector ~ab(t)with respect to body coordinate band 3D orientation of IMU to wco- ordinate, as follows [7] [4], ~!b(t) =!b(t) +bg(t) +\u0011g(t) ~ab(t) =bRw(aw(t) +gb(t)) +ba(t) +\u0011a(t);(1) where bg(t),ba(t)are quasi-constant biases, \u0011g\u0001= N(0;\u001b2 g),\u0011a\u0001= N(0;\u001b2 a)are zero-mean Gaussian noises, bRw2SO(3)is the rotation transforming from frame w tob[20]. Herein, the biases are given following the ran- dom walk process [7][8][20]. Following the kinetic model of holonomic robot, we can compute the instance robot velocity as [3], 2 4vx vy !3 5=\u0019\u001a 42 41 1 1 1 1\u00001 1\u00001 \u00001=d1=d\u00001=d1=d3 52 664!1 !2 !3 !43 775; (2)where vb(t);!bw(t) =3~e2(vx; vy; ! )Tis the current velocity of the robot on SE(3),3~e2indicates the transfor- mation from SE(2) to SE(3) [29], !iis the angular veloc- ity of wheel i,\u001ais the radius of the wheel, and dis the mechanical size of the robot [3]. 3.2 Problem formulation Letxis a sequence state, zis the observation, and uis the control inputs of the robot. The problem is that we need to determine robot states xtgiven z1:tand u1:taligned with observation model uncertainty. We de- note distribution (xt) =p(xtju1:t;z1:t)is the belief state of the robot at time t. Using the Bayes ltering method [16], we can nd the optimal solution to the prob- lem. The prediction step uses the robot motion model p(xtjxt\u00001;ut\u00001)as follows, pre(xt) =Z p(xtjxt\u00001;ut) (xt\u00001)dxt\u00001(3) Then the observation is used to update the prediction state as, (xt) =\u0011p(ztjxt) pre(xt) (4) We assume that all the uncertainties are represented as Gaussian distribution. The most successful stories of Bayes lters are ltering-based methods such as EKF, UKF, PF [16] and factor graph optimization (FGO) [28]. Noted that, AI-based method can learn the motion model (3) and measurement model (4) as the end-to-end learn- ing process. Moreover, we can employ AI to learn the un- certainty that will implement in Bayes lters. We study the learning-based EKF and FGO based-state estimation. Here, an AI technique is used to model the learning problem as, y=gAI( ;\u0012) (5) whereg(:)is the AI solution such as CNN, LSTM, etc.; \u0012 is the trainable weights of the AI solution; yis the Bayes ltering parameter; is the input of the AI technique such as the sensors data, previous states.",
    "word_count": 883
  },
  {
    "chunk_id": 3,
    "text": "Moreover, we can employ AI to learn the un- certainty that will implement in Bayes lters. We study the learning-based EKF and FGO based-state estimation. Here, an AI technique is used to model the learning problem as, y=gAI( ;\u0012) (5) whereg(:)is the AI solution such as CNN, LSTM, etc.; \u0012 is the trainable weights of the AI solution; yis the Bayes ltering parameter; is the input of the AI technique such as the sensors data, previous states. To handle intrinsic sensors data, we employ a Neural Network (NN) as input of a sliding window of W inertial measurements and wheel odometers as, k+1;1= 1( f!i;ai;vb i;!b igW i=k\u0000W+1:k)(6) k+1;2= 2( f!i;ai;vb i;!b igW i=k\u0000W+1:k)(7) where 1and 2are a neural network and LSTM, respec- tively; is the stack function that converts all the input information into a vector; denotes the sequence input into an LSTM; Outputs (:)is the Bayes lter parame- ters, motion model, or measurement model using NN or LSTM. 4. EXPERIMENT EV ALUATION We evaluate a sensor setup consists of two stereo cam- eras, 04 2D LiDARs, 04 wheel encoders, and an IMU, as shown in Fig. 1. The data is recorded from IMU and 13 Authorized licensed use limited to: INSTITUTE OF ENGINEERING & MANAGEMENT TRUST. Downloaded on January 02,2026 at 16:17:09 UTC from IEEE Xplore. Restrictions apply. wheel encoder with 06 Sections. We use multi-sensor fu- sion technique [18] to build the ground truth and learning the observation model from its estimation. A sample sen- sor data in section 3 is shown in Figure 2. 4.1 Learning LSTM and neural networks for mea- surement model We will analyze the LSTM and deep neural net- work for end-to-end learning measurement model fol- lowing [13] [6]. For LSTM, the input is a se- quence of vector f!i;ai;vb i;!b ig(6). The out- puts are a 2-vector [d;\u0012]in a 10-steps sliding win- dow, where d=distance (se2(xk+10\u0000xk)),\u0012= abs(angle (se2(xk+10\u0000xk))). We create an LSTM of two hidden layers as follows [13], layers = [Input(9); bilstmLayer(128, sequence ); bilstmLayer(64, last ); fullyConnectedLayer(32); fullyConnectedLayer(2); regressionLayer]; We use adam algorithm with 1024 epochs for training. After training, the accuracy is 8:73\u000210\u00003MSE. Simi- larly, we design a deep neural network with two hidden networks (7) as, imitateNetwork = [ featureInputLayer(99) fullyConnectedLayer(128) sigmoidLayer( relu ) fullyConnectedLayer(128) sigmoidLayer( relu ) fullyConnectedLayer(2) regressionLayer() ]; The accuracy of the deep neural network is 0.01 MSE. Although we increase the layers of LSTM and DNN, the results for the end-to-end measurement model did not get high accuracy for the holonomic mobile robot, as fol- lowing IoNet [13] and AI-IMU [6]. For the end-to-end method, the results conclude that the LSTM model is bet- ter than DNN. However, when we increase the number of neural for deep learning to 2048 as, imitateNetwork = [ featureInputLayer(99) fullyConnectedLayer(1024) sigmoidLayer( relu ) fullyConnectedLayer(256) sigmoidLayer( relu ) fullyConnectedLayer(2) regressionLayer() ]; The results of DNN about 0.005 is more better than LSTM. Next, we will examine the learning uncertainty model for Bayes lers. 4.2 Learning uncertainty model for Bayes lers The end-to-end solution for the holonomic robot is not working well. Therefore, the learning uncertainty is implemented for Bayes ltering. The input for a net- work is similar aforementioned. The outputs are \u0001p=[\u0001d; \u0001\u0012], where \u0001dis the error of the distance \u0001xto ground-truth, \u0001\u0012is also the error rotation \u0001\u0012to ground- truth. Although the mean squared error (MSE) of LSTM is about 0.01, the actual response posses poor accuracy. The deep learning algorithm is trained on a desktop PC with an Intel 4-core i7-7700 processor CPU and GPU GTX 3070 Ti 8Gb. Next, we design a neural network with two layers (01 hidden layer with 30 neutrals) using Levenberg Marquardt algorithm. The results are surpris- ingly good that provides a 0.005 MSE error for training. Flatten neural network is better than LSTM in the case of learning uncertainty. After training, we will imple- ment the deep neural network into the Bayes lters tech- nique, as shown earlier. Following the type of deep learn- ing accuracy, we will select the attened neural network for learning uncertainty covariance. In contrast, for the learning uncertainty method, the results conclude that the attened neural network is better than LSTM, as shown in Fig. 3. 5. CONCLUSION This paper studies deep neural network based- state estimation using intrinsic sensors for a holonomic mobile robot. We rst provide valuable discussion and oppor- tunities for not only intrinsic sensors fusion but also ex- trinsic sensors merging.",
    "word_count": 986
  },
  {
    "chunk_id": 4,
    "text": "Following the type of deep learn- ing accuracy, we will select the attened neural network for learning uncertainty covariance. In contrast, for the learning uncertainty method, the results conclude that the attened neural network is better than LSTM, as shown in Fig. 3. 5. CONCLUSION This paper studies deep neural network based- state estimation using intrinsic sensors for a holonomic mobile robot. We rst provide valuable discussion and oppor- tunities for not only intrinsic sensors fusion but also ex- trinsic sensors merging. We then show how to connect Bayes ltering to the neural network using multiple sen- sor data information. The initial experiment implemented the LSTM, deep neural networks, and multilayer percep- tron (MLP) to analyze the results. In future work, we will implement the proposed system using speci c state esti- mation techniques such as EFK, PF, FGO. We will also conduct more neural network architecture for evaluation experiments. ACKNOWLEDGEMENT This research was nancially supported in part by the Ministry of Trade, Industry and Energy(MOTIE) and Korea Institute for Advancement of Technology(KIAT) through the International Cooperative R&D program. (Project No.P0004631) and in part by Institute of In- formation & communications Technology Planning & Evaluation (IITP) grant funded by the Korea govern- ment(MSIT) (IITP-2020-0-00211, Development of 5G based swarm autonomous logistics transport technology for smart post of ce) REFERENCES [1] C. Cadena et al., Past, Present, and Future of Si- multaneous Localization and Mapping: Toward the Robust-Perception Age, in IEEE Transactions on Robotics , vol. 32, no. 6, pp. 1309-1332, Dec. 2016. doi: 10.1109/TRO.2016.2624754. [2] Van, N.D et al. A Hierarchical Control System for 14 Authorized licensed use limited to: INSTITUTE OF ENGINEERING & MANAGEMENT TRUST. Downloaded on January 02,2026 at 16:17:09 UTC from IEEE Xplore. Restrictions apply. Fig. 2. A sample dataset was recorded in the real experiment. Fig. 3. The training uncertainty process employs the at- tened neural network of about 0.005 MSE. Autonomous Driving towards Urban Challenges, Appl. Sci. 2020, 10, 3543. [3] Qian, Jun, et al. The design and development of an omni-directional mobile robot oriented to an intel- ligent manufacturing system, Sensors 17.9 (2017): 2073. [4] N. V . Dinh and G. Kim, Multi-sensor Fusion To- wards VINS: A Concise Tutorial, Survey, Frame- work and Challenges, 2020 IEEE International Conference on Big Data and Smart Computing (BigComp) , Busan, Korea (South), 2020, pp. 459- 462. [5] Chen, Changhao, et al. Selective sensor fusion for neural visual-inertial odometry, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019. [6] Brossard, Martin, Axel Barrau, and Silv `ere Bonnabel. AI-IMU dead-reckoning, IEEE Trans- actions on Intelligent Vehicles 5.4 (2020): 585-595. [7] Huang G, Visual-inertial navigation: A concise re- view, In: IEEE Int. Conf. Robot. Autom. (ICRA) , 2019. [8] Patrick Geneva, el al., OpenVINS: A Research Platform for Visual-Inertial Estimation, IROS 2019 Workshop on Visual-Inertial Navigation: Chal-lenges and Applications. [9] Nam, D.V . ; Gon-Woo, K. Robust Stereo Visual Inertial Navigation System Based on Multi-Stage Outlier Removal in Dynamic Environments, Sen- sors 2020, 20, 2922. [10] Shan, Tixiao, et al. Lio-sam: Tightly-coupled li- dar inertial odometry via smoothing and mapping, 2020 IEEE/RSJ International Conference on Intel- ligent Robots and Systems (IROS) . IEEE, 2020. [11] Shan, Tixiao, et al. LVI-SAM: Tightly-coupled Lidar-Visual-Inertial Odometry via Smoothing and Mapping, arXiv preprint arXiv:2104.10831 (2021). [12] Lee, Michelle A., et al. Multimodal sensor fusion with differentiable lters., 2020 IEEE/RSJ Interna- tional Conference on Intelligent Robots and Systems (IROS) . IEEE, 2020. [13] Chen, Changhao, et al. Ionet: Learning to cure the curse of drift in inertial odometry, Proceedings of the AAAI Conference on Arti cial Intelligence . V ol. 32. No. 1. 2018. [14] Zhao, Xiangrui, et al. Learning to Compensate for the Drift and Error of Gyroscope in Vehicle Local- ization, 2020 IEEE Intelligent Vehicles Symposium (IV). IEEE, 2020. [15] Yi, Brent, et al. Differentiable Factor Graph Op- timization for Learning Smoothers, arXiv preprint arXiv:2105.08257 (2021). [16] Barfoot, Timothy D. State estimation for robotics. Cambridge University Press , 2017. [17] Sola, Joan, et al. A micro Lie theory for state estimation in robotics, arXiv preprint arXiv:1812.01537 (2018). [18] Labb e, M, Michaud, F. RTAB-Map as an open- source lidar and visual simultaneous localization and mapping library for large-scale and long-term online operation, J Field Robotics . 2019; 35: 416 446. https://doi.org/10.1002/rob.21831 [19] Kailai Li, Meng Li and Uwe D. Hanebeck. 15 Authorized licensed use limited to: INSTITUTE OF ENGINEERING & MANAGEMENT TRUST. Downloaded on January 02,2026 at 16:17:09 UTC from IEEE Xplore. Restrictions apply.",
    "word_count": 992
  },
  {
    "chunk_id": 5,
    "text": "[18] Labb e, M, Michaud, F. RTAB-Map as an open- source lidar and visual simultaneous localization and mapping library for large-scale and long-term online operation, J Field Robotics . 2019; 35: 416 446. https://doi.org/10.1002/rob.21831 [19] Kailai Li, Meng Li and Uwe D. Hanebeck. 15 Authorized licensed use limited to: INSTITUTE OF ENGINEERING & MANAGEMENT TRUST. Downloaded on January 02,2026 at 16:17:09 UTC from IEEE Xplore. Restrictions apply. Towards High-Performance Solid-State- LiDAR-Inertial Odometry and Mapping, arXiv:2010.13150 . 2020. [20] Forster C, et al., On-manifold preintegra- tion for real-time visual-inertial odome- try, IEEE Trans Robot 33(1):1 21, DOI 10.1109/TRO.2016.2597321, 2017. [21] Frank Dellaert and Michael Kaess, Factor Graphs for Robot Perception, Foundations and Trends in Robotics , V ol. 6, No. 1-2, pp. 1 139, 2017 [22] Van Nam, Dinh, and Kim Gon-Woo. Online Self- Calibration of Multiple 2D LiDARs using Line Fea- tures with Fuzzy Adaptive Covariance, IEEE Sen- sors Journal , vol. 21, no. 12, pp. 13714-13726, 15 June15, 2021, doi: 10.1109/JSEN.2021.3053260. [23] Qin, Tong, et al. Vins-mono: A robust and versa- tile monocular visual-inertial state estimator, IEEE Transactions on Robotics 34.4 (2018): 1004-1020. [24] Liu, Wenxin, et al. TLIO: Tight learned inertial odometry, IEEE Robotics and Automation Letters 5.4 (2020): 5653-5660. [25] Brossard, Martin, et al. RINS-W: Robust inertial navigation system on wheels, 2019 IEEE/RSJ Inter- national Conference on Intelligent Robots and Sys- tems (IROS) . IEEE, 2019. [26] Haarnoja, Tuomas, et al. Backprop kf: Learning discriminative deterministic state estimators, Ad- vances in neural information processing systems . 2016. [27] Herath, Sachini, et al. RoNIN: Robust Neural In- ertial Navigation in the Wild: Benchmark, Evalua- tions, & New Methods, 2020 IEEE International Conference on Robotics and Automation (ICRA) . IEEE, 2020. [28] Dellaert, Frank, and Michael Kaess. Factor graphs for robot perception, Foundations and Trends in Robotics 6.1-2 (2017): 1-139. [29] Geneva, Patrick, et al. Versatile 3d multi-sensor fusion for lightweight 2d localization, 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) . IEEE, 2020. [30] R. Jonschkowski, et al., Differentiable Particle Fil- ters: End-to-End Learning with Algorithmic Pri- ors, in Proceedings of Robotics: Science and Sys- tems (RSS) , 2018. 16 Authorized licensed use limited to: INSTITUTE OF ENGINEERING & MANAGEMENT TRUST. Downloaded on January 02,2026 at 16:17:09 UTC from IEEE Xplore. Restrictions apply.",
    "word_count": 524
  }
]